{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "data_path = config['data_path']\n",
    "\n",
    "corpus_path=data_path+'corpus.csv'\n",
    "print(corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=pd.read_csv(corpus_path)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text=corpus['text'].to_list()\n",
    "corpus_cid=corpus['cid'].to_list()\n",
    "print(corpus_text[0])\n",
    "print(corpus_cid[0])\n",
    "# corpus_tokenization=[doc.split(\" \") for doc in corpus_text]\n",
    "# corpus_tokenization=[word_tokenize(doc) for doc in corpus_text]\n",
    "# print(corpus_tokenization[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path+'tokenized_corpus.pkl', 'rb') as f:\n",
    "    corpus_tokenization = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_bm25 = BM25Okapi(corpus_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(data_path+'train.csv')\n",
    "data_records=train_data.to_dict(orient='records')\n",
    "\n",
    "for point in data_records:\n",
    "    cid_str=point['cid']\n",
    "    point['cid'] = list(map(int, cid_str.strip('[]').split()))\n",
    "print(data_records[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo danh sách tạm để lưu trữ kết quả\n",
    "temp = []\n",
    "# Số lượng điểm tối đa trước khi lưu vào file\n",
    "batch_size = 10\n",
    "\n",
    "for idx, point in enumerate(data_records):\n",
    "    top_relevant = legal_bm25.get_top_n(word_tokenize(point['question']), corpus_cid, 10)\n",
    "    hard_negatives = [item for item in top_relevant if item not in point['cid']]  \n",
    "    curr=[point['qid']]\n",
    "    curr.extend(hard_negatives)\n",
    "    # Thêm question_id và hard_negatives vào danh sách tạm\n",
    "    temp.append(curr)\n",
    "\n",
    "    # Kiểm tra nếu đã đạt đến số lượng batch_size\n",
    "    if (idx + 1) % batch_size == 0:\n",
    "        # Ghi kết quả vào file\n",
    "        with open('hard_negative.txt', 'a') as f:\n",
    "            for item in temp:\n",
    "                f.write(f\"{item}\\n\")\n",
    "        print(f'inserting successfully {idx} elements')\n",
    "        # Xóa danh sách tạm để sẵn sàng cho lần ghi tiếp theo\n",
    "        temp.clear()\n",
    "\n",
    "# Lưu kết quả còn lại sau vòng lặp nếu có\n",
    "if temp:\n",
    "    with open('hard_negative.txt', 'a') as f:\n",
    "        for item in temp:\n",
    "            f.write(f\"{item}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
